{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f077be5a-cfb8-4cbe-b7f4-07e5645fddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Load `num.txt` and `sub.txt` Data from Bronze\n",
    "\n",
    "We load the raw `num.txt` data (numerical facts) and the `sub.txt` file (company metadata) from the Bronze Delta tables. These will be joined during cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3795ec-1df0-48ae-bf5e-7c29f9a81587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option('format','delta')\n",
    "    .load(\"dbfs:/user/hive/warehouse/bronzes.db/numbers\")\n",
    ")\n",
    "\n",
    "sub_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option('format','delta')\n",
    "    .load(\"dbfs:/user/hive/warehouse/bronzes.db/submissions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac99978a-7574-4f7b-b7c6-cca8c5cb503d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Impute Missing `value` Column\n",
    "\n",
    "We fill missing `value`s in `num.txt` by:\n",
    "- Joining with `sub.txt` to get CIK.\n",
    "- Computing median per group (`cik`, `tag`, `uom`).\n",
    "- Imputing nulls using group median.\n",
    "- Flagging rows from groups that had no non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54a3eb2-9330-4771-a6ad-e651e808dee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, count, percentile_approx\n",
    "\n",
    "def impute_num_value_medians(df_num: DataFrame, df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute nulls in the NUM DataFrame’s value column by:\n",
    "      1. Joining to SUB on adsh to bring in cik\n",
    "      2. Grouping by (cik, tag, uom) to compute:\n",
    "         - grp_count  : number of non-null values per group\n",
    "         - grp_median : median of value per group\n",
    "      3. Flagging groups with no non-null values as is_incomplete\n",
    "      4. Filling null value entries with their group’s median where available\n",
    "\n",
    "    Parameters:\n",
    "        df_num (DataFrame): Cleaned NUM DataFrame containing adsh, tag, uom, and value.\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing adsh and cik.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with:\n",
    "          - value nulls imputed by group median\n",
    "          - is_incomplete flag set True for rows in groups with zero non-null values\n",
    "    \"\"\"\n",
    "    # 1) Join NUM → SUB to get CIK\n",
    "    num_joined = df_num.join(\n",
    "        df_sub.select(\"adsh\", \"cik\"),\n",
    "        on=\"adsh\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 2) Compute per-group non-null count & median\n",
    "    group_cols = [\"cik\", \"tag\", \"uom\"]\n",
    "    group_stats = (\n",
    "        num_joined\n",
    "          .filter(col(\"value\").isNotNull())\n",
    "          .groupBy(*group_cols)\n",
    "          .agg(\n",
    "              count(col(\"value\")).alias(\"grp_count\"),\n",
    "              percentile_approx(col(\"value\"), 0.5).alias(\"grp_median\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # 3) Join stats back\n",
    "    df_with_stats = num_joined.join(group_stats, on=group_cols, how=\"left\")\n",
    "\n",
    "    # 4) Flag empty groups & impute medians\n",
    "    df_result = (\n",
    "        df_with_stats\n",
    "          .withColumn(\n",
    "              \"is_value_incomplete\",\n",
    "              when(col(\"grp_count\").isNull(), True).otherwise(False)\n",
    "          )\n",
    "          .withColumn(\n",
    "              \"value\",\n",
    "              when(\n",
    "                  col(\"value\").isNull() & col(\"grp_count\").isNotNull(),\n",
    "                  col(\"grp_median\")\n",
    "              )\n",
    "              .otherwise(col(\"value\"))\n",
    "          )\n",
    "          .drop(\"grp_count\", \"grp_median\")\n",
    "    )\n",
    "\n",
    "    df_result = df_result.drop(\"cik\")\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabf4159-82cd-438d-9b58-03822107d6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_df = impute_num_value_medians(num_df, sub_df)\n",
    "display(num_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99ba2e6-7dca-43d6-8b24-e0ae2f9af42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec6e0d2-f27c-4091-a1e1-55b91b2423de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Final Cleaning & Transformation of `num.txt`\n",
    "\n",
    "- Removed unused columns: `segments`, `coreg`, `footnote`\n",
    "- Removed nulls from critical columns: `value`, `adsh`, `tag`, etc.\n",
    "- Converted `ddate` into proper date format after validating year.\n",
    "- Filtered out rows with abnormal `qtrs` values (should be < 5).\n",
    "- Standardized version to uppercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9a39db-5761-422c-8a6c-9cb92d4ed9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def num_transform(num_df):\n",
    "    num_df = num_df.drop(\"segments\", \"coreg\", \"footnote\")\n",
    "\n",
    "    num_df = num_df.filter(col(\"value\").isNotNull())\n",
    "    \n",
    "    month = substring(col(\"ddate\"), 5, 2)\n",
    "    day = substring(col(\"ddate\"), 7, 2)\n",
    "    original_year = substring(col(\"ddate\"), 1, 4)\n",
    "    original_year_int = original_year.cast(\"int\")\n",
    "    corrected_year = when(\n",
    "    (original_year_int >= 2009) & (original_year_int <= year(current_date())),original_year).otherwise(col(\"year\").cast(\"string\"))\n",
    "\n",
    "    num_df = num_df.withColumn(\"ddate\",to_date(concat_ws(\"\", corrected_year, month, day), \"yyyyMMdd\"))\n",
    "\n",
    "    num_df = num_df.filter(col(\"adsh\").isNotNull() & col(\"tag\").isNotNull() & col(\"version\").isNotNull())\n",
    "\n",
    "    num_df = num_df.withColumn(\"version\", upper(\"version\"))\n",
    "\n",
    "    num_df=num_df.where(col(\"qtrs\") < 5)\n",
    "    num_df=num_df.select(\"adsh\", \"tag\", \"version\", \"ddate\", \"qtrs\", \"uom\", \"value\", \"is_value_incomplete\")\n",
    "    return num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca29f32b-7041-4691-9eed-d61cd511c5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(num_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50029860-0146-4621-b55d-e05b4fc16308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_df=num_transform(num_df)\n",
    "display(num_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4307d1-da55-4efa-bf53-2f8d2a60b738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS silver.numbers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91368982-ee48-4018-b816-aa6a712381c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Save Cleaned Data to Silver Layer\n",
    "\n",
    "The fully transformed and quality-checked data is now written as a Delta Table to the Silver layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d11357-6b12-4122-b718-5dba4fd9cfa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.numbers\")\n",
    "num_df_loaded = spark.read.format(\"delta\").table(\"silver.numbers\")\n",
    "display(num_df_loaded)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7525539422956079,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Numbers_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
