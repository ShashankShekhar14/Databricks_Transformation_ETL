{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a141b6-befe-4557-88df-8869a5006af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load `sub.txt` Data from Bronze Table\n",
    "\n",
    "We start by reading the raw data ingested in the Bronze layer. The data is stored as Delta format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838aa4b6-97b7-434e-bcb0-7a24eea2cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_df = spark.read.option(\"header\", True) \\\n",
    "                   .option(\"delimiter\", \"\\t\") \\\n",
    "                   .option('format','delta')\\\n",
    "                   .load(\"dbfs:/user/hive/warehouse/bronzes.db/submissions\")\n",
    "\n",
    "# sub_df=sub_transform(sub_df)\n",
    "# Display the first 20 rows\n",
    "display(sub_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb9e45b-474f-4707-9402-ef0f0b969a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Impute Categorical Columns (`sic`, `countryba`)\n",
    "\n",
    "This function fills missing values in `sic` and `countryba`:\n",
    "- First by the most frequent value (mode) **per CIK**.\n",
    "- Then using a **global mode** if still missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91a0d21-c152-4795-8d28-2acb05402821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import col, when, row_number\n",
    "\n",
    "def impute_sub_categorical_modes(df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes nulls in the 'sic' and 'countryba' columns of a SUB DataFrame by:\n",
    "      1. Per-CIK mode (most frequent) fill\n",
    "      2. Global mode fallback for any remaining nulls\n",
    "\n",
    "    Parameters:\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing at least 'cik', 'sic', and 'countryba'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with no nulls in 'sic' or 'countryba'.\n",
    "    \"\"\"\n",
    "    df = df_sub\n",
    "    to_impute = [\"sic\", \"countryba\"]\n",
    "\n",
    "    # 1) Per-CIK mode fill\n",
    "    for c in to_impute:\n",
    "        mode_per_cik = (\n",
    "            df\n",
    "              .filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\", c)\n",
    "              .count()\n",
    "              .withColumn(\n",
    "                  \"rn\",\n",
    "                  row_number().over(\n",
    "                      Window.partitionBy(\"cik\")\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "                  )\n",
    "              )\n",
    "              .filter(col(\"rn\") == 1)\n",
    "              .select(\"cik\", col(c).alias(f\"{c}_cik_mode\"))\n",
    "        )\n",
    "        df = (\n",
    "            df\n",
    "              .join(mode_per_cik, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_cik_mode\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_cik_mode\")\n",
    "        )\n",
    "\n",
    "    # 2) Global mode fallback for any remaining nulls\n",
    "    global_modes = {}\n",
    "    for c in to_impute:\n",
    "        mode_val = (\n",
    "            df\n",
    "              .filter(col(c).isNotNull())\n",
    "              .groupBy(c)\n",
    "              .count()\n",
    "              .orderBy(col(\"count\").desc())\n",
    "              .limit(1)\n",
    "              .collect()[0][c]\n",
    "        )\n",
    "        global_modes[c] = mode_val\n",
    "\n",
    "    # 3) Apply global fallback\n",
    "    for c, mode_val in global_modes.items():\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            when(col(c).isNull(), mode_val)\n",
    "            .otherwise(col(c))\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229c558a-12ae-4db7-a6e9-bb2ad18ffd9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_df = impute_sub_categorical_modes(sub_df)\n",
    "display(sub_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "910b5b5a-a43a-46d0-b6db-440ebb56d4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Impute Date Fields (`fy`, `fp`, `period`, `fye`)\n",
    "\n",
    "- For `fy`, `fp`: filled by **mode per CIK**, then fallback values.\n",
    "- For `period`, `fye`: filled using **median per CIK**, then calculated defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43c2c23-0ba0-488e-aa1b-bc4c8bd186dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, row_number,\n",
    "    percentile_approx, to_date,\n",
    "    year, lit\n",
    ")\n",
    "\n",
    "def impute_sub_date_fields(df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute nulls in the SUB DataFrame for the following columns:\n",
    "      - fy, fp       : categorical (mode per CIK, then fallback)\n",
    "      - period, fye  : numeric dates (median per CIK, then fallback)\n",
    "\n",
    "    Parameters:\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing at least\n",
    "                            'cik', 'filed', 'fy', 'fp', 'period', and 'fye'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: New DataFrame with no nulls in 'fy', 'fp', 'period', or 'fye'.\n",
    "    \"\"\"\n",
    "    df = df_sub\n",
    "\n",
    "    # Convert 'filed' (int YYYYMMDD) to date for extracting year\n",
    "    df = df.withColumn(\n",
    "        \"filed_dt\",\n",
    "        to_date(col(\"filed\").cast(\"string\"), \"yyyyMMdd\")\n",
    "    )\n",
    "\n",
    "    # 1) Impute categorical 'fy' and 'fp' by mode per CIK\n",
    "    for c in [\"fy\", \"fp\"]:\n",
    "        mode_df = (\n",
    "            df.filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\", c)\n",
    "              .count()\n",
    "              .withColumn(\n",
    "                  \"rn\",\n",
    "                  row_number().over(\n",
    "                      Window.partitionBy(\"cik\")\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "                  )\n",
    "              )\n",
    "              .filter(col(\"rn\") == 1)\n",
    "              .select(\"cik\", col(c).alias(f\"{c}_mode\"))\n",
    "        )\n",
    "        df = (\n",
    "            df.join(mode_df, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_mode\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_mode\")\n",
    "        )\n",
    "\n",
    "    # 2) Impute numeric 'period' and 'fye' by median per CIK\n",
    "    for c in [\"period\", \"fye\"]:\n",
    "        med_df = (\n",
    "            df.filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\")\n",
    "              .agg(\n",
    "                  percentile_approx(col(c), 0.5).alias(f\"{c}_med\")\n",
    "              )\n",
    "        )\n",
    "        df = (\n",
    "            df.join(med_df, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_med\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_med\")\n",
    "        )\n",
    "\n",
    "    # 3) Final fallback defaults\n",
    "    df = (\n",
    "        df\n",
    "          .withColumn(\"fye\",\n",
    "              when(col(\"fye\").isNull(), lit(1231))\n",
    "              .otherwise(col(\"fye\"))\n",
    "          )\n",
    "          .withColumn(\"fy\",\n",
    "              when(col(\"fy\").isNull(), year(col(\"filed_dt\")))\n",
    "              .otherwise(col(\"fy\"))\n",
    "          )\n",
    "          .withColumn(\"period\",\n",
    "              when(col(\"period\").isNull(), col(\"fy\") * 10000 + col(\"fye\"))\n",
    "              .otherwise(col(\"period\"))\n",
    "          )\n",
    "          .withColumn(\"fp\",\n",
    "              when(col(\"fp\").isNull(), lit(\"FY\"))\n",
    "              .otherwise(col(\"fp\"))\n",
    "          )\n",
    "          .drop(\"filed_dt\")\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e31dcb5-aa33-4532-8391-1a8ae3e492c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_df = impute_sub_date_fields(sub_df)\n",
    "display(sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcd7960-3447-4766-aa15-9a4c1ea41c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Final Cleaning and Transformation\n",
    "\n",
    "- Dropped irrelevant columns.\n",
    "- Converted date formats.\n",
    "- Removed special characters.\n",
    "- Added `delay_days` column (difference between `filed` and `period`).\n",
    "- Trimmed whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24df1c2-f014-42ce-be33-b4e292ca21d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace, trim, datediff\n",
    "\n",
    "def sub_transform(sub_df):\n",
    "    sub_df = sub_df.filter(col(\"adsh\").isNotNull() & col(\"cik\").isNotNull() & col(\"name\").isNotNull() & col(\"sic\").isNotNull())\n",
    "\n",
    "    sub_df = sub_df.drop(\"bas1\", \"bas2\", \"countryma\", \"stprma\", \"cityma\", \"zipma\", \"mas1\", \"mas2\", \"countryinc\", \"stprinc\", \"ein\", \"former\", \"changed\", \"afs\", \"wksi\", \"prevrpt\", \"detail\", \"nciks\", \"aciks\")\n",
    "\n",
    "    sub_df = sub_df.withColumn(\"period\", to_date(\"period\", \"yyyyMMdd\"))\n",
    "    sub_df = sub_df.withColumn(\"filed\", to_date(\"filed\", \"yyyyMMdd\"))\n",
    "    sub_df = sub_df.drop(\"year\",\"quarter\")\n",
    "    sub_df = sub_df.withColumn(\"baph\", regexp_replace(\"baph\", \"[^0-9]\", \"\"))\n",
    "\n",
    "    columns_to_trim = [\"name\", \"countryba\", \"stprba\", \"cityba\", \"zipba\", \"form\", \"instance\"]\n",
    "    for col_name in columns_to_trim:\n",
    "        sub_df = sub_df.withColumn(col_name, trim(col_name))\n",
    "    \n",
    "    sub_df=sub_df.withColumn(\"delay_days\", datediff(col(\"filed\"), col(\"period\")))\n",
    "\n",
    "    sub_df = sub_df.select(\n",
    "        \"adsh\", \"cik\", \"name\", \"sic\", \"countryba\", \"stprba\", \"cityba\", \"zipba\", \"baph\",\n",
    "        \"fye\", \"form\", \"period\", \"fy\", \"fp\", \"filed\", \"delay_days\", \"accepted\", \"instance\"\n",
    "    )\n",
    "    \n",
    "    return sub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99360836-9892-4424-b57e-fcc8e3679f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_df=sub_transform(sub_df)\n",
    "# Display the first 20 rows\n",
    "display(sub_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1413a49a-a7f0-4e4e-aa65-f5d92be27300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Check if nulls are present in either 'sic' or 'countryba'\n",
    "# sub_df.filter(col(\"sic\").isNull() | col(\"countryba\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b5f9f2-3db4-40c0-9cf2-aeea74591548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Check if nulls are present in either 'sic' or 'countryba'\n",
    "# sub_df.filter(col(\"fye\").isNull() | col(\"fp\").isNull() | col(\"fy\").isNull() | col(\"period\").isNull()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66463ab8-cd23-4a76-810b-0475e286dde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists silver.submissions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f818fc5-a7cf-4acc-be47-5482c8c62e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.submissions\")\n",
    "sub_df_loaded = spark.read.format(\"delta\").table(\"silver.submissions\")\n",
    "display(sub_df_loaded)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7525539422956087,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Submissions_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
