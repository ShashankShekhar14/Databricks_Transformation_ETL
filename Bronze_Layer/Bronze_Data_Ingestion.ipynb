{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8144d59d-2155-48e7-9de8-7cf86668530a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c4b883-b1ea-4041-ab8b-f26625824e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_value = dbutils.secrets.get('connection-credentials', 'secret-value')\n",
    "application_id = dbutils.secrets.get('connection-credentials', 'application-id')\n",
    "directory_id = dbutils.secrets.get('connection-credentials', 'app-registration-directory-id')\n",
    "storage_account = dbutils.secrets.get('connection-credentials', 'storage-account-name')\n",
    "container_name = dbutils.secrets.get('connection-credentials', 'container-name')\n",
    "security_key = dbutils.secrets.get('connection-credentials', 'storage-account-security-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47166e2-74ed-4584-9efa-71dbbd82ba2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaad915c-91cb-433e-bc41-aec682f3a1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", secret_value)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723dc006-b0f9-46d7-aa02-41a4ad8e7b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Mount the container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d49683-4869-4e6c-9ee0-796a0637f7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net\",\n",
    "  mount_point = f\"/mnt/{container_name}\",\n",
    "  extra_configs = {\n",
    "    f\"fs.azure.account.key.storageaccb1.blob.core.windows.net\":security_key\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ab090b-a994-471d-add9-b090e26b6e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load the data in Bronze layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00cca4b-5157-4f14-be79-c79ba135dffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ParquetToDelta\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "root_directory = \"/mnt/unzip-financial-data\"\n",
    "\n",
    "# Ensure the bronze_delta schema exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronzes\")\n",
    "\n",
    "# Schemas\n",
    "submissions_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), False),\n",
    "    StructField(\"cik\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"sic\", IntegerType(), True),\n",
    "    StructField(\"countryba\", StringType(), False),\n",
    "    StructField(\"stprba\", StringType(), True),\n",
    "    StructField(\"cityba\", StringType(), False),\n",
    "    StructField(\"zipba\", StringType(), True),\n",
    "    StructField(\"bas1\", StringType(), True),\n",
    "    StructField(\"bas2\", StringType(), True),\n",
    "    StructField(\"baph\", StringType(), True),\n",
    "    StructField(\"countryma\", StringType(), True),\n",
    "    StructField(\"stprma\", StringType(), True),\n",
    "    StructField(\"cityma\", StringType(), True),\n",
    "    StructField(\"zipma\", StringType(), True),\n",
    "    StructField(\"mas1\", StringType(), True),\n",
    "    StructField(\"mas2\", StringType(), True),\n",
    "    StructField(\"countryinc\", StringType(), False),\n",
    "    StructField(\"stprinc\", StringType(), True),\n",
    "    StructField(\"ein\", StringType(), True),\n",
    "    StructField(\"former\", StringType(), True),\n",
    "    StructField(\"changed\", DateType(), True),\n",
    "    StructField(\"afs\", StringType(), True),\n",
    "    StructField(\"wksi\", BooleanType(), False),\n",
    "    StructField(\"fye\", StringType(), False),\n",
    "    StructField(\"form\", StringType(), False),\n",
    "    StructField(\"period\", StringType(), False),\n",
    "    StructField(\"fy\", IntegerType(), False),\n",
    "    StructField(\"fp\", StringType(), False),\n",
    "    StructField(\"filed\", StringType(), False),\n",
    "    StructField(\"accepted\", TimestampType(), False),\n",
    "    StructField(\"prevrpt\", BooleanType(), False),\n",
    "    StructField(\"detail\", BooleanType(), False),\n",
    "    StructField(\"instance\", StringType(), False),\n",
    "    StructField(\"nciks\", IntegerType(), False),\n",
    "    StructField(\"aciks\", StringType(), True)\n",
    "])\n",
    "\n",
    "tags_schema = StructType([\n",
    "    StructField(\"tag\", StringType(), False),\n",
    "    StructField(\"version\", StringType(), False),\n",
    "    StructField(\"custom\", IntegerType(), False),\n",
    "    StructField(\"abstract\", BooleanType(), False),\n",
    "    StructField(\"datatype\", StringType(), True),\n",
    "    StructField(\"iord\", StringType(), True),\n",
    "    StructField(\"crdr\", StringType(), True),\n",
    "    StructField(\"tlabel\", StringType(), True),\n",
    "    StructField(\"doc\", StringType(), True)\n",
    "])\n",
    "\n",
    "numbers_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"ddate\", StringType(), True),\n",
    "    StructField(\"qtrs\", IntegerType(), True),\n",
    "    StructField(\"uom\", StringType(), True),\n",
    "    StructField(\"coreg\", StringType(), True),\n",
    "    StructField(\"value\", DecimalType(28,4), True),\n",
    "    StructField(\"footnote\", StringType(), True)\n",
    "])\n",
    "\n",
    "presentations_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), False),\n",
    "    StructField(\"report\", IntegerType(), True),\n",
    "    StructField(\"line\", IntegerType(), False),\n",
    "    StructField(\"stmt\", StringType(), False),\n",
    "    StructField(\"inpth\", BooleanType(), False),\n",
    "    StructField(\"rfile\", StringType(), False),\n",
    "    StructField(\"tag\", StringType(), False),\n",
    "    StructField(\"version\", StringType(), False),\n",
    "    StructField(\"plabel\", StringType(), False)\n",
    "])\n",
    "\n",
    "def extract_year_quarter(folder_path):\n",
    "    match = re.search(r'(\\d{4})[^\\d]*q([1-4])', folder_path, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))\n",
    "    return None, None\n",
    "def process_quarter_folder(folder_path):\n",
    "    year, quarter = extract_year_quarter(folder_path)\n",
    "    if not year or not quarter:\n",
    "        print(f\"Skipping folder {folder_path} due to invalid naming convention.\")\n",
    "        return\n",
    "\n",
    "    files = dbutils.fs.ls(folder_path)\n",
    "    for file in files:\n",
    "        file_path = file.path\n",
    "        if file.name == 'pre.parquet':\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in presentations_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df = df.withColumn(\"year\", lit(year)).withColumn(\"quarter\", lit(quarter))\n",
    "            if not spark._jsparkSession.catalog().tableExists(\"bronzes\", \"Presentations\"):\n",
    "                df.write.format(\"delta\").mode(\"append\").partitionBy(\"year\", \"quarter\").saveAsTable(\"bronzes.Presentations\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronzes.Presentations\")\n",
    "\n",
    "        elif file.name == 'num.parquet':\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in numbers_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df = df.withColumn(\"year\", lit(year)).withColumn(\"quarter\", lit(quarter))\n",
    "            if not spark._jsparkSession.catalog().tableExists(\"bronzes\", \"Numbers\"):\n",
    "                df.write.format(\"delta\").mode(\"append\").partitionBy(\"year\", \"quarter\").saveAsTable(\"bronzes.Numbers\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronzes.Numbers\")\n",
    "\n",
    "        elif file.name == 'sub.parquet':\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in submissions_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df = df.withColumn(\"year\", lit(year)).withColumn(\"quarter\", lit(quarter))\n",
    "            if not spark._jsparkSession.catalog().tableExists(\"bronzes\", \"Submissions\"):\n",
    "                df.write.format(\"delta\").mode(\"append\").partitionBy(\"year\", \"quarter\").saveAsTable(\"bronzes.Submissions\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronzes.Submissions\")\n",
    "\n",
    "        elif file.name == 'tag.parquet':\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in tags_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df = df.withColumn(\"year\", lit(year)).withColumn(\"quarter\", lit(quarter))\n",
    "            if not spark._jsparkSession.catalog().tableExists(\"bronzes\", \"Tags\"):\n",
    "                df.write.format(\"delta\").mode(\"append\").partitionBy(\"year\", \"quarter\").saveAsTable(\"bronzes.Tags\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronzes.Tags\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping file {file.name} as it doesn't match any known pattern\")\n",
    "\n",
    "def process_all_quarters():\n",
    "    quarters = dbutils.fs.ls(root_directory)\n",
    "    for quarter in quarters:\n",
    "        print(f\"Processing folder: {quarter.name}\")\n",
    "        process_quarter_folder(quarter.path)\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "print(f\"Starting to process all quarter folders from {root_directory}\")\n",
    "process_all_quarters()\n",
    "print(\"Data upload completed for all quarters.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7565462627216503,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Data_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
