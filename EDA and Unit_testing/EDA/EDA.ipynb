{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b628dd3e-aa9d-4510-a793-0830de4e6b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5578be8-e197-490e-9f4e-cbb7ada83c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import col, count, isnan, isnull, countDistinct, sum, when\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "num = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(\"dbfs:/FileStore/Data/2024q1/num.txt\")\n",
    "\n",
    "tag = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(\"dbfs:/FileStore/Data/2024q1/tag.txt\")\n",
    "\n",
    "pre = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(\"dbfs:/FileStore/Data/2024q1/pre.txt\")\n",
    "\n",
    "sub = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(\"dbfs:/FileStore/Data/2024q1/sub.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36acdb6a-b239-4b8c-ab58-d5ec45537ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining the generate_df_info function for df summary generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34038952-5d15-46be-9a69-ca5757f13e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resulting DF contains\n",
    "* **`cols`**\n",
    "\n",
    "  * **Description**: The name of each column in the original DataFrame.\n",
    "\n",
    "* **`data_types`**\n",
    "\n",
    "  * **Description**: The Spark‐inferred data type for that column (e.g., “string,” “int,” “timestamp”).\n",
    "\n",
    "* **`null_percentage`**\n",
    "\n",
    "  * **Description**: The percentage of rows in which that column is null (empty).\n",
    "  * **Importance**: Shows us how incomplete a column is; if it’s mostly null, we might drop it or treat it differently in your pipeline.\n",
    "\n",
    "* **`distinct_count`**\n",
    "\n",
    "  * **Description**: The number of unique, non‐null values that column contains.\n",
    "  * **Importance**: Helps us understand cardinality. A low distinct count often means a small set of categories (good for dimension tables).\n",
    "\n",
    "* **`top1_value`**\n",
    "\n",
    "  * **Description**: The single most frequent (non‐null) value that appears in that column.\n",
    "  * **Importance**: Quickly tells you if one category dominates. For example, if “USP” appears 95% of the time, that column may not be very informative for analysis.\n",
    "\n",
    "* **`top1_value_count`**\n",
    "\n",
    "  * **Description**: How many times the `top1_value` appears.\n",
    "  * **Importance**: Paired with `distinct_count`, it shows us how skewed the distribution is. A high count relative to total rows indicates low variability.\n",
    "\n",
    "* **`top1_value_percentage`**\n",
    "\n",
    "  * **Description**: The `top1_value_count` expressed as a percentage of total rows.\n",
    "  * **Importance**: Gives immediate context for how dominant the top value is (e.g., 80% means four out of five rows share that same value, suggesting low diversity in that column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02bc0a0-b9fb-4bdd-8e59-e5cd869965ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def generate_df_info(df):\n",
    "    \"\"\"\n",
    "    Given any Spark DataFrame `df`, returns a summary DataFrame with:\n",
    "      - cols\n",
    "      - data_types\n",
    "      - null_percentage\n",
    "      - distinct_count\n",
    "      - top1_value\n",
    "      - top1_value_count\n",
    "      - top1_value_percentage\n",
    "    \"\"\"\n",
    "    # 1. Total row count\n",
    "    total_rows = df.count()\n",
    "\n",
    "    # 2. Computing null counts per column (only isNull)\n",
    "    null_exprs = [\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nullCount\")\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    null_counts_dict = df.select(*null_exprs).collect()[0].asDict()\n",
    "\n",
    "    # 3. Build initial rows: (col_name, dtype_str, null_pct)\n",
    "    log_rows = []\n",
    "    for col_name, dtype_str in df.dtypes:\n",
    "        null_count = null_counts_dict[f\"{col_name}_nullCount\"]\n",
    "        null_pct = float(null_count) * 100.0 / total_rows\n",
    "        log_rows.append((col_name, dtype_str, null_pct))\n",
    "\n",
    "    # 4. Defining schema for core df_info\n",
    "    core_schema = StructType([\n",
    "        StructField(\"cols\", StringType(), nullable=False),\n",
    "        StructField(\"data_types\", StringType(), nullable=False),\n",
    "        StructField(\"null_percentage\", DoubleType(), nullable=False)\n",
    "    ])\n",
    "    df_info = spark.createDataFrame(log_rows, schema=core_schema)\n",
    "\n",
    "    # 5. Computing distinct (non-null) count per column\n",
    "    distinct_exprs = [\n",
    "        countDistinct(col(c)).alias(f\"{c}_distinctCount\")\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    distinct_counts_dict = df.select(*distinct_exprs).collect()[0].asDict()\n",
    "\n",
    "    distinct_rows = [\n",
    "        (column.replace(\"_distinctCount\", \"\"), distinct_counts_dict[column])\n",
    "        for column in distinct_counts_dict\n",
    "    ]\n",
    "    distinct_schema = StructType([\n",
    "        StructField(\"cols\", StringType(), nullable=False),\n",
    "        StructField(\"distinct_count\", IntegerType(), nullable=False)\n",
    "    ])\n",
    "    distinct_df = spark.createDataFrame(distinct_rows, schema=distinct_schema)\n",
    "\n",
    "    # 6. Computing top-1 value, count, and percentage for string columns\n",
    "    string_cols = [name for name, dtype in df.dtypes if dtype == \"string\"]\n",
    "    top1_rows = []\n",
    "    for c in string_cols:\n",
    "        top_row = (\n",
    "            df.filter(col(c).isNotNull())\n",
    "              .groupBy(c).count()\n",
    "              .orderBy(desc(\"count\"))\n",
    "              .limit(1)\n",
    "              .collect()\n",
    "        )\n",
    "        if top_row:\n",
    "            val = top_row[0][c]\n",
    "            cnt = top_row[0][\"count\"]\n",
    "            pct = float(cnt) * 100.0 / total_rows\n",
    "        else:\n",
    "            val, cnt, pct = None, 0, 0.0\n",
    "        top1_rows.append((c, val, cnt, pct))\n",
    "\n",
    "    top1_schema = StructType([\n",
    "        StructField(\"cols\", StringType(), nullable=False),\n",
    "        StructField(\"top1_value\", StringType(), nullable=True),\n",
    "        StructField(\"top1_value_count\", IntegerType(), nullable=False),\n",
    "        StructField(\"top1_value_percentage\", DoubleType(), nullable=False)\n",
    "    ])\n",
    "    top1_df = spark.createDataFrame(top1_rows, schema=top1_schema)\n",
    "\n",
    "    # 7. Joining everything together on \"cols\"\n",
    "    df_info = df_info \\\n",
    "        .join(distinct_df, on=\"cols\", how=\"left\") \\\n",
    "        .join(top1_df, on=\"cols\", how=\"left\")\n",
    "\n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5d65f6-ed56-4245-a27f-6b3506033ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c5bab5-e958-4a8c-8b52-6dd230f9ff46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Submissions DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae763c2-9856-4f9a-ae91-0322101d7115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"sub DF row count = \", sub.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187b607f-4e5a-4603-b6a8-eb0fde71b2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### INSPECT SCHEMA & DATA TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d194c3b6-c5e8-4369-9c7b-15018ddd9870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"---- Schema of sub ----\")\n",
    "sub.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93298dfb-1ba9-4f46-b66f-95a2f801abd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Sub_Info Summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae638f4-2b8d-473e-bc4d-6a9d073d6a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_info = generate_df_info(sub)\n",
    "sub_info.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6eddf5-5472-4951-8800-5612444b30e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "sub_file:\n",
    "\"adsh\", \"cik\", \"name\", \"sic\", \"countryba\", \"stprba\", \"cityba\", \"zipba\", \"baph\",\n",
    "\"fye\", \"form\", \"period\", \"fy\", \"fp\", \"filed\", \"delay_days\", \"accepted\", \"instance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "281164f6-8ea3-499a-8a0c-de66884ec6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. **Column Selection for Gold**\n",
    "\n",
    "   * If a column’s `distinct_count` is extremely high and we only need a few metrics, so we might drop that column from the Gold table (too many distinct values can bloat your fact tables).\n",
    "   * If a column’s `null_percentage` > 50%, might be that it’s too sparse to include as a Gold attribute and either fill it or drop it.\n",
    "\n",
    "2. **Data‐Type Corrections**\n",
    "\n",
    "   * The `note` column already flags “Convert to datetime” or “Convert to bool.” Once converted, we can set `is_good = True` for those rows in the next run.\n",
    "\n",
    "3. **Dimension Strategy**\n",
    "\n",
    "   * Low‐cardinality string columns (distinct\\_count < 50) are good candidates for dimension tables.\n",
    "\n",
    "4. **Automated Alerts**\n",
    "\n",
    "   * If `top1_count / total_rows` is > 90% for a column (i.e., one value dominates 90% of records), we might want to examine whether the column is actually useful or should be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1999baa-ffcb-4b85-8476-79536f02e6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Numbers DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1860d07-4b6a-4a0f-b615-b1490fee46e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"sub DF row count = \", num.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3fc5fa0-cb12-4822-bf97-c141dc61f36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"---- Schema of sub ----\")\n",
    "num.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e5bcc0-2a1e-48aa-a192-3d0c7cfdd3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_info = generate_df_info(num)\n",
    "num_info.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ab0b39-1906-4dce-b09a-4a34d8241d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "num_file:\n",
    "adsh\n",
    "tag\n",
    "version\n",
    "ddate\n",
    "qtrs\n",
    "uom\n",
    "value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e15f830-fc6f-4b0b-bc1e-1cf8b1cac1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num.where(col(\"adsh\") == \"0001161697-24-000084\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52927881-5db2-4324-9f72-a6fe60bfb44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### One to many connection of SUB and NUM\n",
    "Each SUB row is like a “folder” for one filing, and that folder can contain **hundreds or even thousands** of individual numbers. For example, a single 10-K report might tag and report:\n",
    "\n",
    "* **Balance sheet line items**: cash, receivables, inventory, property, debt…\n",
    "* **Income statement items**: revenue, cost of goods sold, operating expenses, net income…\n",
    "* **Cash flow items**: operating cash flow, investing cash flow, financing cash flow…\n",
    "* **Footnotes and segments**: interest expense by segment, tax footnotes, etc.\n",
    "\n",
    "Each of those tagged numbers becomes **one row** in NUM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f2cea4-7597-4bca-85c4-d6a6a857bc71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Quarter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c85941-9d47-48e5-9b7a-af2d022c1048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num.groupBy(\"qtrs\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "930e0cc6-e382-444a-8afb-e27d41254970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Values are mostly recorded as \"Point-in-time\"(0) or \"for a year\"(4)\n",
    "- in vary rare cases we see the use of values other than 0,1,2,3 and 4 which might be an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "602f899f-0045-4e9c-aee8-44247df5b48c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Units of measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8547d1d-d470-4aa6-ba36-314e44d60320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num.groupBy(\"uom\").agg(count(\"*\").alias(\"count\"), (count(\"*\")*100/num.count()).alias(\"percentage rows covered\")).orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1ce9e49-7929-4a85-bdcd-fe5ca8947835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Description for each of the units \n",
    "\n",
    "* **USD**: United States dollars (the standard currency unit for most financial facts).\n",
    "* **shares**: Number of equity shares outstanding or issued.\n",
    "* **pure**: A unitless, “pure” numeric value (no currency or other unit).\n",
    "* **CAD**: Canadian dollars.\n",
    "* **EUR**: Euros (the common currency of the Eurozone).\n",
    "* **GBP**: British pounds sterling.\n",
    "* **CNY**: Chinese yuan renminbi.\n",
    "* **BRL**: Brazilian reais.\n",
    "* **CHF**: Swiss francs.\n",
    "* **CLP**: Chilean pesos.\n",
    "* **COP**: Colombian pesos.\n",
    "* **Rate**: A proportion or percentage rate (e.g., interest or growth rate).\n",
    "* **AUD**: Australian dollars.\n",
    "* **JPY**: Japanese yen.\n",
    "* **DKK**: Danish kroner.\n",
    "* **SEK**: Swedish kronor.\n",
    "* **PHP**: Philippine pesos.\n",
    "* **HKD**: Hong Kong dollars.\n",
    "* **ILS**: Israeli new shekels.\n",
    "* **SGD**: Singapore dollars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3395984c-f71c-4f5e-874e-401697650ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_table = num.summary()\n",
    "summary_table.select(\"summary\", \"value\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93dee4b-0de3-4b70-99a9-633f7d930952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see values in both positives and negatives, depicting that '-' negative sign is being used to measure cretid from the repective company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98fbc6e4-6854-455e-b494-8c22699f533e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inferences on NUM\n",
    "\n",
    "1. **Tag cardinality vs. “hot” tags**\n",
    "\n",
    "   * **`tag`** has **65 705** distinct values—an extremely long tail of seldom-used tags. The single most common tag, **`StockholdersEquity`**, appears \\~102 770 times (3 %).\n",
    "   * **Inference**: You’ll want to **filter** in Silver to a curated list of “Gold” tags (e.g., top 20 by frequency) rather than ingest every one of the \\~65 000 tags into your main fact table.\n",
    "\n",
    "4. **Quarter vs. date granularity**\n",
    "\n",
    "   * **`qtrs`** shows 67 distinct values—more than the expected {0,1,2,3,4}. Any other numbers are not valid under the XBRL definition\n",
    "   * **Inference**: We have to implement a cleaning transformation step in Silver to enforce that `qtrs ∈ {0,1,2,3,4}` and that `ddate` falls on end-of-quarter.\n",
    "\n",
    "5. **Unit of measure concentration**\n",
    "\n",
    "   * **`uom`** has 89 distinct units, but **84 %** of rows are **`USD`**; the rest include “shares,” “pure,” and dozens of other currencies.\n",
    "   * **Inference**:\n",
    "\n",
    "     * **Filter** out non-USD if our client only care about dollar amounts.\n",
    "\n",
    "6. **Sparse dimension columns**\n",
    "\n",
    "   * **`segments`** is \\~45 % null.\n",
    "   * **`coreg`** is \\~99 % null (only a handful of co-registrant cases).\n",
    "   * **`footnote`** is \\~99.8 % null.\n",
    "   * **Inference**:\n",
    "\n",
    "     * Treat `coreg` and `footnote` as **“detail”** fields that belong in a secondary table (e.g., only join when the user asks for footnotes).\n",
    "\n",
    "7. **Value coverage and cleaning**\n",
    "\n",
    "   * **`value`** (cast to `double`) has a small null rate (3.6 % of rows failed to convert).\n",
    "   * **Inference**:\n",
    "\n",
    "     * Add a Silver step to **clean** the `value` field (strip non-numeric characters, catch “NM” or “—” cases) before final cast.\n",
    "     * We may Consoder dropping rows where `value` remains null after cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b17046c-98e0-4a59-83a8-cc4731edcb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A Look on NULLS in num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "197f5fa8-4f49-41cc-b87a-eb20617abf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Nulls vs Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ea9497-c445-47ef-ac3f-33ed85510ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtering the null value rows from the df\n",
    "null_vals = num.filter(col(\"value\").isNull())\n",
    "\n",
    "# Total number of null-value rows\n",
    "total_nulls = null_vals.count()\n",
    "print(f\"Total null-value rows: {total_nulls}\")\n",
    "\n",
    "# TAGs have with most nulls\n",
    "null_by_tag = (\n",
    "    null_vals.groupBy(\"tag\")\n",
    "             .agg(count(\"*\").alias(\"null_count\"))\n",
    ")\n",
    "\n",
    "total_by_tag = num.groupBy(\"tag\").agg(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "tag_null_summary = (\n",
    "    null_by_tag\n",
    "      .join(total_by_tag, on=\"tag\", how=\"inner\")\n",
    "      .withColumn(\"null_pct_of_tag\", round(col(\"null_count\") / col(\"total_count\") * 100, 2))\n",
    "      .orderBy(col(\"null_count\").desc())\n",
    ")\n",
    "\n",
    "print(\"=================== Top 10 tags by # of null values ==================\")\n",
    "tag_null_summary.select(\"tag\", \"null_count\", \"total_count\", \"null_pct_of_tag\") \\\n",
    "                .display(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7302d765-922f-48c2-bf95-72eaa7989493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Taking tags which have more than 50 percent nulls and ordering them by null count (desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b9c6eb-4d73-4e0d-9365-1b087576022c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag_null_summary.select(\"tag\", \"null_count\", \"total_count\", \"null_pct_of_tag\").where(col(\"null_pct_of_tag\") >= 50).orderBy(col(\"null_count\").desc()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62f43a6-a8f0-4e05-9db7-a114f858783d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb694e61-adf0-42e4-aeff-954a7ab3f34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Nulls vs Units of Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991f5dd2-30f8-4873-9ce6-67dd35b487ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_by_uom = (\n",
    "    null_vals.groupBy(\"uom\")\n",
    "             .agg(count(\"*\").alias(\"null_count\"))\n",
    ")\n",
    "total_by_uom = num.groupBy(\"uom\").agg(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "uom_null_summary = (\n",
    "    null_by_uom\n",
    "      .join(total_by_uom, on=\"uom\", how=\"inner\")\n",
    "      .withColumn(\"null_pct_of_uom\",\n",
    "                  round(col(\"null_count\") / col(\"total_count\") * 100, 2))\n",
    "      .orderBy(col(\"null_count\").desc())\n",
    ")\n",
    "\n",
    "print(\"================ Top 10 UOMs by # of null values =============\")\n",
    "uom_null_summary.select(\"uom\", \"null_count\", \"total_count\", \"null_pct_of_uom\").display(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37fd9b68-bf84-4454-b7b1-802a2f4f6581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Nulls vs Quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6d7bf5-5db8-4241-9931-4e4c7f6ea52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "null_by_qtrs = (\n",
    "    null_vals.groupBy(\"qtrs\")\n",
    "             .agg(count(\"*\").alias(\"null_count\"))\n",
    ")\n",
    "total_by_qtrs = num.groupBy(\"qtrs\").agg(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "qtrs_null_summary = (\n",
    "    null_by_qtrs\n",
    "      .join(total_by_qtrs, on=\"qtrs\", how=\"inner\")\n",
    "      .withColumn(\"null_pct_of_qtrs\",\n",
    "                  round(col(\"null_count\") / col(\"total_count\") * 100, 2))\n",
    "      .orderBy(col(\"null_count\").desc())\n",
    ")\n",
    "\n",
    "print(\"================ Null-value breakdown by qtrs =================\")\n",
    "qtrs_null_summary.select(\"qtrs\", \"null_count\", \"total_count\", \"null_pct_of_qtrs\").display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6064a3ae-2fcf-4d29-87fd-bebc697edb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d4e487-f94e-45ce-a51a-bab8d3717012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Tag DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b73a8a-d3b3-4302-8f7b-ad1f851ac6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"sub DF row count = \", tag.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb690fa-166a-4629-a2e8-984c5842f376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"---- Schema of sub ----\")\n",
    "tag.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa3a836-459c-4463-a8ac-08419311dc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag_info = generate_df_info(tag)\n",
    "tag_info.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "689a0007-f1b9-4f61-900c-6ec637e2ff81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "tag_file:\n",
    "tag_id\n",
    "tag\n",
    "version\n",
    "datatype\n",
    "iord\n",
    "tlabel\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce12e561-6d77-44c3-b9e7-b181080fd5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom tag proportion in TAG table VS. Custom tags usage in NUM table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91b8f90-c576-4d04-84d9-2fff1df53ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag.groupby(\"custom\").agg(count(\"*\").alias(\"count\"), (count(\"*\")*100/tag.count()).alias(\"percentage\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aebb51bc-fca4-4244-a6bb-341dfb979db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- This 79993 non-custom tags were filed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f727e-3a92-4ee2-9bea-6d4df82831c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_tag_joined = num.join(\n",
    "    tag.select(\"tag\", \"version\", \"custom\", \"abstract\"),\n",
    "    on=[\"tag\", \"version\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "num_tag_joined.groupby(\"custom\").agg(count(\"*\").alias(\"count\"), (count(\"*\")*100/num_tag_joined.count()).alias(\"percentage\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b534eaf-f85c-4b2f-ac41-4f275c32e189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Although Custom tags are 92.4 percent of the definitions but only use 8.8 percent of the numbers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a74660-6858-4124-91fa-6b2d5614c212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf174a2-a09e-426d-8be8-d8b11a57bfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba65e37-7f3d-47ad-9f13-4df57f87e4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Presentation DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de74da94-3580-48eb-ad2b-bd821469587f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"sub DF row count = \", pre.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b710c09-0ee1-46e9-a4e6-7b543d91b5d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"---- Schema of sub ----\")\n",
    "pre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805a1588-44c5-4d1c-bb85-212f12ce992c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pre_info = generate_df_info(pre)\n",
    "pre_info.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc2305f-f669-4f4b-bb9d-687713889a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "pre_file:\n",
    "adsh\n",
    "report\n",
    "line\n",
    "stmt\n",
    "tag\n",
    "version\n",
    "plabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a4506c-dfc7-4491-82d1-06a5137e3180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub.select(\"form\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60a3f49-d6ad-4322-ac46-8b6bbcf68428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag.groupBy(\"datatype\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcaceb8-8806-422b-b7a8-abe13681145f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag.where(col(\"datatype\") != \"monetary\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47767cc0-7b33-4341-b8a3-9e28fa82af61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30713692-9cba-4a81-a5f9-3ff2da10613b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2215388c-c656-40af-b996-77568fa0bf69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1097907e-3e95-44f4-9c37-f1c27ddb079c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sub nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdce0970-36e2-4435-82c0-2c1d0d5428f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### sic and countryba\n",
    "- Per-CIK mode: filling nulls with each company’s most common value for that field.\n",
    "\n",
    "- Global fallback mode: for any CIK that has no non-null values, we fill with the overall most frequent value in the entire table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f19f373-9aec-43b3-87f6-0e1693dd6f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import col, when, row_number\n",
    "\n",
    "def impute_sub_categorical_modes(df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes nulls in the 'sic' and 'countryba' columns of a SUB DataFrame by:\n",
    "      1. Per-CIK mode (most frequent) fill\n",
    "      2. Global mode fallback for any remaining nulls\n",
    "\n",
    "    Parameters:\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing at least 'cik', 'sic', and 'countryba'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with no nulls in 'sic' or 'countryba'.\n",
    "    \"\"\"\n",
    "    df = df_sub\n",
    "    to_impute = [\"sic\", \"countryba\"]\n",
    "\n",
    "    # 1) Per-CIK mode fill\n",
    "    for c in to_impute:\n",
    "        mode_per_cik = (\n",
    "            df\n",
    "              .filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\", c)\n",
    "              .count()\n",
    "              .withColumn(\n",
    "                  \"rn\",\n",
    "                  row_number().over(\n",
    "                      Window.partitionBy(\"cik\")\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "                  )\n",
    "              )\n",
    "              .filter(col(\"rn\") == 1)\n",
    "              .select(\"cik\", col(c).alias(f\"{c}_cik_mode\"))\n",
    "        )\n",
    "        df = (\n",
    "            df\n",
    "              .join(mode_per_cik, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_cik_mode\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_cik_mode\")\n",
    "        )\n",
    "\n",
    "    # 2) Global mode fallback for any remaining nulls\n",
    "    global_modes = {}\n",
    "    for c in to_impute:\n",
    "        mode_val = (\n",
    "            df\n",
    "              .filter(col(c).isNotNull())\n",
    "              .groupBy(c)\n",
    "              .count()\n",
    "              .orderBy(col(\"count\").desc())\n",
    "              .limit(1)\n",
    "              .collect()[0][c]\n",
    "        )\n",
    "        global_modes[c] = mode_val\n",
    "\n",
    "    # 3) Apply global fallback\n",
    "    for c, mode_val in global_modes.items():\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            when(col(c).isNull(), mode_val)\n",
    "            .otherwise(col(c))\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "261a8633-10d7-4244-9271-076d47f4d271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "753f1f56-2c81-413e-893a-cb6866178f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee5e554-5742-4488-84c1-23449c6363fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### fye, fy, period, fp\n",
    "\n",
    "##### following are default\n",
    "\n",
    "- fye → 1231 (Dec 31)\n",
    "\n",
    "- fy → the filing’s calendar year (extracted from the filed date)\n",
    "\n",
    "- period → fiscal-year end date in YYYYMMDD form, i.e. fy * 10000 + fye\n",
    "\n",
    "- fp → \"FY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dda632b-73f8-43ae-a257-3c6a2c16e63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, row_number,\n",
    "    percentile_approx, to_date,\n",
    "    year, lit\n",
    ")\n",
    "\n",
    "def impute_sub_date_fields(df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute nulls in the SUB DataFrame for the following columns:\n",
    "      - fy, fp       : categorical (mode per CIK, then fallback)\n",
    "      - period, fye  : numeric dates (median per CIK, then fallback)\n",
    "\n",
    "    Parameters:\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing at least\n",
    "                            'cik', 'filed', 'fy', 'fp', 'period', and 'fye'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: New DataFrame with no nulls in 'fy', 'fp', 'period', or 'fye'.\n",
    "    \"\"\"\n",
    "    df = df_sub\n",
    "\n",
    "    # Convert 'filed' (int YYYYMMDD) to date for extracting year\n",
    "    df = df.withColumn(\n",
    "        \"filed_dt\",\n",
    "        to_date(col(\"filed\").cast(\"string\"), \"yyyyMMdd\")\n",
    "    )\n",
    "\n",
    "    # 1) Impute categorical 'fy' and 'fp' by mode per CIK\n",
    "    for c in [\"fy\", \"fp\"]:\n",
    "        mode_df = (\n",
    "            df.filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\", c)\n",
    "              .count()\n",
    "              .withColumn(\n",
    "                  \"rn\",\n",
    "                  row_number().over(\n",
    "                      Window.partitionBy(\"cik\")\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "                  )\n",
    "              )\n",
    "              .filter(col(\"rn\") == 1)\n",
    "              .select(\"cik\", col(c).alias(f\"{c}_mode\"))\n",
    "        )\n",
    "        df = (\n",
    "            df.join(mode_df, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_mode\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_mode\")\n",
    "        )\n",
    "\n",
    "    # 2) Impute numeric 'period' and 'fye' by median per CIK\n",
    "    for c in [\"period\", \"fye\"]:\n",
    "        med_df = (\n",
    "            df.filter(col(c).isNotNull())\n",
    "              .groupBy(\"cik\")\n",
    "              .agg(\n",
    "                  percentile_approx(col(c), 0.5).alias(f\"{c}_med\")\n",
    "              )\n",
    "        )\n",
    "        df = (\n",
    "            df.join(med_df, on=\"cik\", how=\"left\")\n",
    "              .withColumn(\n",
    "                  c,\n",
    "                  when(col(c).isNull(), col(f\"{c}_med\"))\n",
    "                  .otherwise(col(c))\n",
    "              )\n",
    "              .drop(f\"{c}_med\")\n",
    "        )\n",
    "\n",
    "    # 3) Final fallback defaults\n",
    "    df = (\n",
    "        df\n",
    "          .withColumn(\"fye\",\n",
    "              when(col(\"fye\").isNull(), lit(1231))\n",
    "              .otherwise(col(\"fye\"))\n",
    "          )\n",
    "          .withColumn(\"fy\",\n",
    "              when(col(\"fy\").isNull(), year(col(\"filed_dt\")))\n",
    "              .otherwise(col(\"fy\"))\n",
    "          )\n",
    "          .withColumn(\"period\",\n",
    "              when(col(\"period\").isNull(), col(\"fy\") * 10000 + col(\"fye\"))\n",
    "              .otherwise(col(\"period\"))\n",
    "          )\n",
    "          .withColumn(\"fp\",\n",
    "              when(col(\"fp\").isNull(), lit(\"FY\"))\n",
    "              .otherwise(col(\"fp\"))\n",
    "          )\n",
    "          .drop(\"filed_dt\")\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "711ff107-2bca-4051-822c-e858730aade7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We pick **mode** for the categorical fields (`fy` and `fp`) and **median** for the numeric‐date fields (`period` and `fye`) because of the nature of those columns:\n",
    "\n",
    "1. **fy (Fiscal Year) and fp (Fiscal Period)**\n",
    "\n",
    "   * Those are **labels**, not numbers you’d average.\n",
    "   * You want the single year or period code a company most often uses, so filling with the **most frequent** value (the mode) makes sense.\n",
    "   * Example: if Acme Corp. has five filings and four of them say `fp = \"Q2\"`, then any missing `fp` should almost certainly be `\"Q2\"`, not `\"Q1\"` or `\"FY\"`.\n",
    "\n",
    "2. **period (Reporting Date) and fye (Fiscal Year-End Month/Day)**\n",
    "\n",
    "   * These are **numeric dates** stored as integers. Averaging them can produce nonsensical halfway dates (e.g. a “.5” day), and means are easily skewed by one outlier.\n",
    "   * The **median** finds the exact middle of a company’s historical dates, so if one quarter was filed very late or early, it won’t drag your fill date to that extreme.\n",
    "   * Example: if your company usually files around June 30 but had one odd March 31 filing, the median will stay at June 30 rather than shift toward March.\n",
    "\n",
    "In short:\n",
    "\n",
    "* **Mode** → best for categorical or discrete codes you want “most common.”\n",
    "* **Median** → best for continuous or ordered data (like dates) where you need a robust center point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "092c4bd6-af9d-4277-b214-1c3349c97b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cb17725-eb08-4618-93d5-933b76283fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff471f20-a437-4e02-ae14-3c6d9379c075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc3f7fa0-9d8d-4fc3-a7b0-9a03637194b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aeb9fbf-5e27-4b89-9a92-032f6539c9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932b7d8a-3bfe-4dc9-88a7-ddc93cc2ee7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b10ad16-4c85-4cb4-b8cc-d9997e4a59ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, count, percentile_approx\n",
    "\n",
    "def impute_num_value_medians(df_num: DataFrame, df_sub: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute nulls in the NUM DataFrame’s `value` column by:\n",
    "      1. Joining to SUB on `adsh` to bring in `cik`\n",
    "      2. Grouping by (cik, tag, uom) to compute:\n",
    "         - grp_count  : number of non-null values per group\n",
    "         - grp_median : median of value per group\n",
    "      3. Flagging groups with no non-null values as `is_incomplete`\n",
    "      4. Filling null `value` entries with their group’s median where available\n",
    "\n",
    "    Parameters:\n",
    "        df_num (DataFrame): Cleaned NUM DataFrame containing `adsh`, `tag`, `uom`, and `value`.\n",
    "        df_sub (DataFrame): Cleaned SUB DataFrame containing `adsh` and `cik`.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with:\n",
    "          - `value` nulls imputed by group median\n",
    "          - `is_incomplete` flag set True for rows in groups with zero non-null values\n",
    "    \"\"\"\n",
    "    # 1) Join NUM → SUB to get CIK\n",
    "    num_joined = df_num.join(\n",
    "        df_sub.select(\"adsh\", \"cik\"),\n",
    "        on=\"adsh\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 2) Compute per-group non-null count & median\n",
    "    group_cols = [\"cik\", \"tag\", \"uom\"]\n",
    "    group_stats = (\n",
    "        num_joined\n",
    "          .filter(col(\"value\").isNotNull())\n",
    "          .groupBy(*group_cols)\n",
    "          .agg(\n",
    "              count(col(\"value\")).alias(\"grp_count\"),\n",
    "              percentile_approx(col(\"value\"), 0.5).alias(\"grp_median\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # 3) Join stats back\n",
    "    df_with_stats = num_joined.join(group_stats, on=group_cols, how=\"left\")\n",
    "\n",
    "    # 4) Flag empty groups & impute medians\n",
    "    df_result = (\n",
    "        df_with_stats\n",
    "          .withColumn(\n",
    "              \"is_incomplete\",\n",
    "              when(col(\"grp_count\").isNull(), True).otherwise(False)\n",
    "          )\n",
    "          .withColumn(\n",
    "              \"value\",\n",
    "              when(\n",
    "                  col(\"value\").isNull() & col(\"grp_count\").isNotNull(),\n",
    "                  col(\"grp_median\")\n",
    "              )\n",
    "              .otherwise(col(\"value\"))\n",
    "          )\n",
    "          .drop(\"grp_count\", \"grp_median\")\n",
    "    )\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adf95f09-bbad-458c-893d-fc6826541aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71b36cd-aee8-49a7-a616-87f0d30b0359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa83423-6d1e-4016-a644-a496ee0fda00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e265573-21ad-4ed9-8be9-79068664d327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### stmt \n",
    "1. Compute each tag’s most common stmt (its mode) across all rows.\n",
    "\n",
    "2. Join that back onto PRE and fill any null stmt with the tag’s mode.\n",
    "\n",
    "3. Fallback to \"UN\" (Unclassifiable) for any tag that never appeared with a non-null stmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bb18fc-d02d-4081-88fb-d50a9d4fd684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.functions import col, when, row_number\n",
    "\n",
    "def impute_pre_stmt(df_pre: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Impute nulls in the PRE DataFrame’s `stmt` column by:\n",
    "      1. Computing each tag’s most frequent statement type (mode) across filings.\n",
    "      2. Filling null `stmt` values with that per-tag mode.\n",
    "      3. As a final fallback, assigning 'UN' (Unclassifiable) to any remaining nulls.\n",
    "\n",
    "    Parameters:\n",
    "        df_pre (DataFrame): Cleaned PRE DataFrame containing at least 'tag' and 'stmt'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with no nulls in `stmt`.\n",
    "    \"\"\"\n",
    "    # 1) Build per-tag mode lookup for stmt\n",
    "    tag_mode_stmt = (\n",
    "        df_pre\n",
    "          .filter(col(\"stmt\").isNotNull())\n",
    "          .groupBy(\"tag\", \"stmt\")\n",
    "          .count()\n",
    "          .withColumn(\n",
    "              \"rn\",\n",
    "              row_number().over(\n",
    "                  Window.partitionBy(\"tag\")\n",
    "                        .orderBy(col(\"count\").desc())\n",
    "              )\n",
    "          )\n",
    "          .filter(col(\"rn\") == 1)\n",
    "          .select(\"tag\", col(\"stmt\").alias(\"stmt_mode\"))\n",
    "    )\n",
    "\n",
    "    # 2) Left-join and fill with per-tag mode\n",
    "    df_filled = (\n",
    "        df_pre\n",
    "          .join(tag_mode_stmt, on=\"tag\", how=\"left\")\n",
    "          .withColumn(\n",
    "              \"stmt\",\n",
    "              when(col(\"stmt\").isNull(), col(\"stmt_mode\"))\n",
    "              .otherwise(col(\"stmt\"))\n",
    "          )\n",
    "          .drop(\"stmt_mode\")\n",
    "    )\n",
    "\n",
    "    # 3) Final fallback: assign 'UN' to any still-null stmt\n",
    "    df_result = df_filled.withColumn(\n",
    "        \"stmt\",\n",
    "        when(col(\"stmt\").isNull(), \"UN\")\n",
    "        .otherwise(col(\"stmt\"))\n",
    "    )\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "454e8917-e86b-4256-b192-20956b84552a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, regexp_replace, initcap\n",
    "\n",
    "def impute_plabel_from_tag(df_pre: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fill null 'plabel' values in a PRE DataFrame by deriving a label from the 'tag':\n",
    "      - Inserts spaces before capital letters that follow lowercase letters\n",
    "      - Converts the result to title case\n",
    "\n",
    "    Parameters:\n",
    "        df_pre (DataFrame): Cleaned PRE DataFrame containing 'tag' and 'plabel'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with no nulls in 'plabel'.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df_pre\n",
    "          .withColumn(\n",
    "              \"plabel\",\n",
    "              when(\n",
    "                  col(\"plabel\").isNull(),\n",
    "                  initcap(\n",
    "                      regexp_replace(col(\"tag\"), \"([a-z])([A-Z])\", \"$1 $2\")\n",
    "                  )\n",
    "              )\n",
    "              .otherwise(col(\"plabel\"))\n",
    "          )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6700eae3-b3bb-4c7d-b6c6-fd164a11862e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f688d1b-1c88-426a-a135-f75eb98ab4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36809580-c520-40d0-88ed-c122a4e81b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db77ed74-5028-4e6d-9930-f19216b90c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b89309-47f0-4d2c-99c3-8d49f77c9e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aeda5d5-763f-45d7-8a4b-5411125b7a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef7d87e-945f-4eba-aff5-81c1933d0917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
