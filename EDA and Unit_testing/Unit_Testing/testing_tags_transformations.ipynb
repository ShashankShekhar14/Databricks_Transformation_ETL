{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "450fd4b0-0330-460d-91f6-a8dda60f96f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pytest chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c083ee28-7e9f-4bb6-91db-1012d8e1857f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from chispa.dataframe_comparer import assert_df_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2a61042d-8490-4833-b922-21b8288cb41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tag_transform(tag_df):\n",
    "    tag_df = tag_df.filter(tag_df[\"version\"].isNotNull())\n",
    "\n",
    "    tag_df = tag_df.withColumn(\"version\", upper(\"version\"))\n",
    "\n",
    "    # Define a dummy window over the entire dataset (no partitioning)\n",
    "    windowSpec = Window.orderBy(\"tag\", \"version\")\n",
    "\n",
    "    # Add surrogate key column\n",
    "    tag_df = tag_df.withColumn(\"tag_id\", row_number().over(windowSpec))\n",
    "\n",
    "    tag_df = tag_df.where(\"abstract != 1\")\n",
    "\n",
    "    tag_df = tag_df.drop(\"custom\", \"abstract\", \"crdr\")\n",
    "\n",
    "    # Get all columns except tag_id\n",
    "    cols = [c for c in tag_df.columns if c != \"tag_id\"]\n",
    "\n",
    "    # Reorder columns with tag_id first\n",
    "    tag_df = tag_df.select([\"tag_id\"] + cols)\n",
    "\n",
    "    return tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "045e1acd-7783-4c26-ba9d-5b132c7aefc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"custom\", IntegerType(), True),\n",
    "    StructField(\"abstract\", BooleanType(), True),\n",
    "    StructField(\"datatype\", StringType(), True),\n",
    "    StructField(\"iord\", StringType(), True),\n",
    "    StructField(\"crdr\", StringType(), True),\n",
    "    StructField(\"tlabel\", StringType(), True),\n",
    "    StructField(\"doc\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"tag_id\", IntegerType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"datatype\", StringType(), True),\n",
    "    StructField(\"iord\", StringType(), True),\n",
    "    StructField(\"tlabel\", StringType(), True),\n",
    "    StructField(\"doc\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "])\n",
    "sample_data = [\n",
    "    # Valid row – should be included\n",
    "    (\"Rev\", \"us-gaap\", 0, False, \"monetary\", \"I\", \"CR\", \"Revenue\", \"Revenue line\", 2024, 1),\n",
    "\n",
    "    # Null version – should be excluded\n",
    "    (\"Loss\", None, 1, False, \"monetary\", \"I\", \"DR\", \"Net Loss\", \"Loss line\", 2024, 1),\n",
    "\n",
    "    # Abstract is True – should be excluded\n",
    "    (\"Asset\", \"ifrs\", 0, True, \"monetary\", \"I\", \"CR\", \"Asset Value\", \"Assets line\", 2024, 1),\n",
    "\n",
    "    # Extra valid row with lowercase version\n",
    "    (\"Equity\", \"ifrs\", 0, False, \"monetary\", \"C\", \"DR\", \"Equity Val\", \"Equity line\", 2024, 1),\n",
    "\n",
    "    # Duplicate version-tag with different iord/tlabel\n",
    "    (\"Rev\", \"us-gaap\", 0, False, \"monetary\", \"A\", \"CR\", \"Revenue Alt\", \"Revenue Alt line\", 2024, 1),\n",
    "]\n",
    "\n",
    "expected_data = [\n",
    "    (1, \"Equity\", \"IFRS\", \"monetary\", \"C\", \"Equity Val\", \"Equity line\", 2024, 1),\n",
    "    (2, \"Rev\", \"US-GAAP\", \"monetary\", \"A\", \"Revenue Alt\", \"Revenue Alt line\", 2024, 1),\n",
    "    (3, \"Rev\", \"US-GAAP\", \"monetary\", \"I\", \"Revenue\", \"Revenue line\", 2024, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a78b2d5-3a6a-48fe-9bdc-ba0774030b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark():\n",
    "    return SparkSession.builder.appName(\"TagTransformTests\").getOrCreate()\n",
    "\n",
    "def test_tag_transformation_basic(spark):\n",
    "    print(\"\\nRunning basic tag transformation test...\")\n",
    "    input_df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n",
    "    result_df = tag_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        # Ignore surrogate key value, just check the rest match\n",
    "        assert_df_equality(result_df.drop(\"tag_id\"), expected_df.drop(\"tag_id\"), ignore_row_order=True)\n",
    "        print(\"✅ Basic transformation test passed\")\n",
    "    except AssertionError as e:\n",
    "        print(\"❌ Basic transformation test failed\")\n",
    "        raise e\n",
    "\n",
    "def test_null_version_filtered(spark):\n",
    "    print(\"\\nRunning version null filtering test...\")\n",
    "    df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    result_df = tag_transform(df)\n",
    "\n",
    "    try:\n",
    "        assert result_df.filter(col(\"version\").isNull()).count() == 0\n",
    "        print(\"✅ Null version filtering test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Null version filtering test failed\")\n",
    "        raise\n",
    "\n",
    "def test_version_uppercase(spark):\n",
    "    print(\"\\nRunning version uppercasing test...\")\n",
    "    df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    result_df = tag_transform(df)\n",
    "\n",
    "    try:\n",
    "        versions = [row[\"version\"] for row in result_df.select(\"version\").collect()]\n",
    "        assert all(v == v.upper() for v in versions)\n",
    "        print(\"✅ Version uppercasing test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Version uppercasing test failed\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed7427e-93be-430f-bced-d9285a766bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_session = spark()\n",
    "test_tag_transformation_basic(spark_session)\n",
    "test_null_version_filtered(spark_session)\n",
    "test_version_uppercase(spark_session)\n",
    "print(\"\\n✅ All `tag_transform` tests completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "testing_tags_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
