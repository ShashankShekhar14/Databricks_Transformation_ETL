{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9eb8178e-de73-40fb-bca2-a378f8be99a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pytest chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec37f7b-7fe1-4f8f-9d6d-d8245fe774b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from decimal import Decimal\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DateType,\n",
    "    IntegerType, DecimalType\n",
    ")\n",
    "from chispa.dataframe_comparer import assert_df_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f3af10-1596-411e-8cf0-10f82d9bcb48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def num_transform(num_df):\n",
    "    num_df = num_df.drop(\"segments\", \"coreg\", \"footnote\")\n",
    "\n",
    "    num_df = num_df.filter(col(\"value\").isNotNull())\n",
    "    \n",
    "    month = substring(col(\"ddate\"), 5, 2)\n",
    "    day = substring(col(\"ddate\"), 7, 2)\n",
    "    original_year = substring(col(\"ddate\"), 1, 4)\n",
    "    original_year_int = original_year.cast(\"int\")\n",
    "    corrected_year = when(\n",
    "    (original_year_int >= 2009) & (original_year_int <= year(current_date())),original_year).otherwise(col(\"year\").cast(\"string\"))\n",
    "\n",
    "    num_df = num_df.withColumn(\"ddate\",to_date(concat_ws(\"\", corrected_year, month, day), \"yyyyMMdd\"))\n",
    "\n",
    "    num_df = num_df.filter(col(\"ddate\").isNotNull())\n",
    "\n",
    "    num_df = num_df.filter(col(\"adsh\").isNotNull() & col(\"tag\").isNotNull() & col(\"version\").isNotNull())\n",
    "\n",
    "    num_df = num_df.withColumn(\"version\", upper(\"version\"))\n",
    "    num_df = num_df.drop(\"year\", \"quarter\")\n",
    "    return num_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343509c9-667c-4f2f-9e54-c3f2cbbe5b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    (\"0001\", \"Tag1\", \"us-gaap\", \"20320101\", 1, \"USD\", \"seg1\", \"coreg1\", Decimal(\"100.0000\"), \"note1\", 2022, 1),\n",
    "    (\"0002\", \"Tag2\", \"ifrs\", \"19920102\", 2, \"EUR\", None, None, Decimal(\"200.0000\"), None, 2022, 1),\n",
    "    (None, \"Tag3\", \"us-gaap\", \"20220103\", 3, \"USD\", \"seg2\", \"coreg2\", Decimal(\"300.0000\"), \"note2\", 2022, 1),\n",
    "    (\"0004\", None, \"ifrs\", \"20220104\", 4, \"EUR\", \"seg3\", \"coreg3\", None, \"note3\", 2022, 1),\n",
    "    (\"0005\", \"Tag5\", None, \"19970105\", 5, \"USD\", \"seg4\", \"coreg4\", Decimal(\"500.0000\"), \"note4\", 2022, 1)\n",
    "]\n",
    "\n",
    "expected_data = [\n",
    "    (\"0001\", \"Tag1\", \"US-GAAP\", date(2022, 1, 1), 1, \"USD\", Decimal(\"100.0000\")),\n",
    "    (\"0002\", \"Tag2\", \"IFRS\", date(2022, 1, 2), 2, \"EUR\", Decimal(\"200.0000\"))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"ddate\", StringType(), True),\n",
    "    StructField(\"qtrs\", IntegerType(), True),\n",
    "    StructField(\"uom\", StringType(), True),\n",
    "    StructField(\"segments\", StringType(), True),\n",
    "    StructField(\"coreg\", StringType(), True),\n",
    "    StructField(\"value\", DecimalType(28, 4), True),\n",
    "    StructField(\"footnote\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"ddate\", DateType(), True),\n",
    "    StructField(\"qtrs\", IntegerType(), True),\n",
    "    StructField(\"uom\", StringType(), True),\n",
    "    StructField(\"value\", DecimalType(28, 4), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119a1f16-a813-4a6b-abec-85d5cf8c6da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark():\n",
    "    return SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "\n",
    "# Basic transformation test\n",
    "def test_basic_transformation(spark):\n",
    "    print(\"\\nRunning basic transformation test...\")\n",
    "\n",
    "    input_df = spark.createDataFrame(sample_data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_schema)\n",
    "    transformed_df = num_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert_df_equality(transformed_df, expected_df, ignore_nullable=True)\n",
    "        print(\"✅ Basic transformation test passed\")\n",
    "    except AssertionError as e:\n",
    "        print(\"❌ Basic transformation test failed\")\n",
    "        raise e\n",
    "\n",
    "# Null value filtering\n",
    "def test_null_value_filtering(spark):\n",
    "    print(\"\\nRunning null value filtering test...\")\n",
    "\n",
    "    test_data = [(\"0001\", \"Tag1\", \"us-gaap\", \"20220101\", 1, \"USD\", None, None, None, None, 2022, 1)]\n",
    "    input_df = spark.createDataFrame(test_data, schema)\n",
    "    transformed_df = num_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert transformed_df.count() == 0\n",
    "        print(\"✅ Null value filtering test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Null value filtering test failed\")\n",
    "        raise\n",
    "\n",
    "# Required fields filtering\n",
    "def test_required_fields_filtering(spark):\n",
    "    print(\"\\nRunning required fields filtering test...\")\n",
    "\n",
    "    test_data = [\n",
    "        (None, \"Tag1\", \"us-gaap\", \"20220101\", 1, \"USD\", None, None, Decimal(\"100.0\"), None, 2022, 1),\n",
    "        (\"0002\", None, \"us-gaap\", \"20220101\", 1, \"USD\", None, None, Decimal(\"100.0\"), None, 2022, 1),\n",
    "        (\"0003\", \"Tag3\", None, \"20220101\", 1, \"USD\", None, None, Decimal(\"100.0\"), None, 2022, 1)\n",
    "    ]\n",
    "    input_df = spark.createDataFrame(test_data, schema)\n",
    "    transformed_df = num_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert transformed_df.count() == 0\n",
    "        print(\"✅ Required fields filtering test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Required fields filtering test failed\")\n",
    "        raise\n",
    "\n",
    "# Version to uppercase test\n",
    "def test_version_uppercase(spark):\n",
    "    print(\"\\nRunning version uppercase test...\")\n",
    "\n",
    "    test_data = [(\"0001\", \"Tag1\", \"us-gaap\", \"20220101\", 1, \"USD\", None, None, Decimal(\"100.0\"), None, 2022, 1)]\n",
    "    input_df = spark.createDataFrame(test_data, schema)\n",
    "    transformed_df = num_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert transformed_df.first()[\"version\"] == \"US-GAAP\"\n",
    "        print(\"✅ Version uppercase test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Version uppercase test failed\")\n",
    "        raise\n",
    "\n",
    "# Date conversion test\n",
    "def test_date_conversion(spark):\n",
    "    print(\"\\nRunning date conversion test...\")\n",
    "\n",
    "    test_data = [(\"0001\", \"Tag1\", \"us-gaap\", \"20220101\", 1, \"USD\", None, None, Decimal(\"100.0\"), None, 2022, 1)]\n",
    "    input_df = spark.createDataFrame(test_data, schema)\n",
    "    transformed_df = num_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert str(transformed_df.first()[\"ddate\"]) == \"2022-01-01\"\n",
    "        print(\"✅ Date conversion test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Date conversion test failed\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c60c57-ec18-47f7-b417-fd0dac23c2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_session = spark()\n",
    "test_basic_transformation(spark_session)\n",
    "test_null_value_filtering(spark_session)\n",
    "test_required_fields_filtering(spark_session)\n",
    "test_version_uppercase(spark_session)\n",
    "test_date_conversion(spark_session)\n",
    "print(\"\\nAll tests completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4988830339339819,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "testing_number_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
