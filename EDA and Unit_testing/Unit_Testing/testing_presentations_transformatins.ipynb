{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "48313ab5-28f8-4336-acf6-95147b4c1b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pytest chispa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b5c59f91-42bb-4208-90da-f16468c4f7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from chispa.dataframe_comparer import assert_df_equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8c677496-f694-4379-aa74-917311f9eb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pre_transform(pre_df):\n",
    "    pre_df=pre_df.drop(\"inpth\", \"rfile\", \"negating\")\n",
    "    pre_df=pre_df.filter(col(\"stmt\").isNotNull())\n",
    "    \n",
    "    columns_to_trim = [\"adsh\", \"stmt\", \"tag\", \"version\", \"plabel\"]\n",
    "    for col_name in columns_to_trim:\n",
    "        pre_df = pre_df.withColumn(col_name, trim(col_name))\n",
    "    return pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910f12c3-bfee-4142-99f3-5644d45c74ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True),\n",
    "    StructField(\"report\", IntegerType(), True),\n",
    "    StructField(\"line\", IntegerType(), True),\n",
    "    StructField(\"stmt\", StringType(), True),\n",
    "    StructField(\"inpth\", BooleanType(), True),\n",
    "    StructField(\"rfile\", StringType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"plabel\", StringType(), True),\n",
    "    StructField(\"negating\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True),\n",
    "    StructField(\"report\", IntegerType(), True),\n",
    "    StructField(\"line\", IntegerType(), True),\n",
    "    StructField(\"stmt\", StringType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"plabel\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "sample_data = [\n",
    "    (\" 0001 \", 1, 10, \" BS \", True, \"R1\", \" Tag1 \", \" Ver1 \", \" Label1 \", \"N\", 2024, 1),   # Valid, needs trim\n",
    "    (\"0002\", 2, 20, None, False, \"R2\", \"Tag2\", \"Ver2\", \"Label2\", \"Y\", 2024, 1),            # Invalid: stmt is null\n",
    "    (\"0003\", 3, 30, \"IS\", True, \"R3\", \" Tag3 \", \" ver3 \", \" Label3 \", \"N\", 2023, 4),       # Valid, needs trim and case check\n",
    "    (\" 0004\", 4, 40, \" CF \", False, \"R4\", \"Tag4\", \"Ver4\", \"Label4\", \"N\", 2022, 3),         # Valid, needs trim\n",
    "    (\"0005\", 5, 50, \"EQ\", True, \"R5\", \" Tag5 \", None, \" Label5 \", \"Y\", 2022, 2),           # Valid, version is null (keep None)\n",
    "    (\"0006\", 6, 60, \"BS\", False, \"R6\", \"Tag6\", \"Ver6\", \"Label6\", None, 2022, 1),           # Valid, no trimming needed\n",
    "    (\"0007\", 7, 70, \"RE\", True, \"R7\", \"Tag7\", \"Ver7\", \"Label7\", \"Y\", None, None),          # Valid, null year/quarter\n",
    "]\n",
    "\n",
    "expected_data = [\n",
    "    (\"0001\", 1, 10, \"BS\", \"Tag1\", \"Ver1\", \"Label1\", 2024, 1),\n",
    "    (\"0003\", 3, 30, \"IS\", \"Tag3\", \"ver3\", \"Label3\", 2023, 4),\n",
    "    (\"0004\", 4, 40, \"CF\", \"Tag4\", \"Ver4\", \"Label4\", 2022, 3),\n",
    "    (\"0005\", 5, 50, \"EQ\", \"Tag5\", None, \"Label5\", 2022, 2),\n",
    "    (\"0006\", 6, 60, \"BS\", \"Tag6\", \"Ver6\", \"Label6\", 2022, 1),\n",
    "    (\"0007\", 7, 70, \"RE\", \"Tag7\", \"Ver7\", \"Label7\", None, None),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4a475b-c52f-4db3-9dc0-04f4354a97c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark():\n",
    "    return SparkSession.builder.appName(\"PreTransformTests\").getOrCreate()\n",
    "\n",
    "def test_pre_transformation_basic(spark):\n",
    "    print(\"\\nRunning basic pre transformation test...\")\n",
    "    input_df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n",
    "    result_df = pre_transform(input_df)\n",
    "\n",
    "    try:\n",
    "        assert_df_equality(result_df, expected_df, ignore_nullable=True)\n",
    "        print(\"✅ Basic transformation test passed\")\n",
    "    except AssertionError as e:\n",
    "        print(\"❌ Basic transformation test failed\")\n",
    "        raise e\n",
    "\n",
    "def test_null_stmt_filtered(spark):\n",
    "    print(\"\\nRunning stmt null filtering test...\")\n",
    "    df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    result_df = pre_transform(df)\n",
    "\n",
    "    try:\n",
    "        assert result_df.filter(col(\"stmt\").isNull()).count() == 0\n",
    "        print(\"✅ Null stmt filtering test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Null stmt filtering test failed\")\n",
    "        raise\n",
    "\n",
    "def test_trim_columns(spark):\n",
    "    print(\"\\nRunning whitespace trimming test...\")\n",
    "    df = spark.createDataFrame(sample_data, schema=schema)\n",
    "    result_df = pre_transform(df)\n",
    "\n",
    "    try:\n",
    "        row = result_df.first()\n",
    "        assert row[\"adsh\"] == \"0001\"\n",
    "        assert row[\"stmt\"] == \"BS\"\n",
    "        assert row[\"tag\"] == \"Tag1\"\n",
    "        assert row[\"version\"] == \"Ver1\"\n",
    "        assert row[\"plabel\"] == \"Label1\"\n",
    "        print(\"✅ Whitespace trimming test passed\")\n",
    "    except AssertionError:\n",
    "        print(\"❌ Whitespace trimming test failed\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f74bd8-bebf-4ec1-b7ec-28df5a55713f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_session = spark()\n",
    "test_pre_transformation_basic(spark_session)\n",
    "test_null_stmt_filtered(spark_session)\n",
    "test_trim_columns(spark_session)\n",
    "print(\"\\n✅ All `pre_transform` tests completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "testing_presentations_transformatins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
